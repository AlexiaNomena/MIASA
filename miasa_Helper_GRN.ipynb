{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b26c252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Changes to local modules might not update if Kernel is not restarted'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Errors might remain if Kernel is not restarted\"\"\"\n",
    "\"\"\"Changes to local modules might not update if Kernel is not restarted\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada83d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Necessary modules \"\"\"\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from Methods.classify import split_data, plotClass, plotClass_separated\n",
    "from Methods.miasa_class import Miasa_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d395ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Name and origin of dataset \"\"\"\n",
    "DataName = \"GRN_data\"\n",
    "from Methods.simulate_class_data import load_data_twoGRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8b84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Euclidean embedding pameters only used in MIASA (includes a finite number of auto adjustements)\n",
    "    \n",
    "    if custom, then type dictionary \n",
    "    c_dic = {\"c1\":float, \"c2\":float, \"c3\":float} \n",
    "\"\"\"\n",
    "c_dic = \"default\" \n",
    "\n",
    "\"\"\" Load or Generate data: \n",
    "    \n",
    "    Required:\n",
    "    X and Y are separated datasets with M, N samples, respectively, with each samples containing K realizations\n",
    "    X.shape = (M, K) \n",
    "    Y.shape = (N, K)\n",
    "    num_clust = number of clusters\n",
    "    dtp : tuple (datatype X_vars , datatype Y_vars) is needed for visualization only\n",
    "    \n",
    "\n",
    "    Not Required:\n",
    "    X_vars, Y_vars = Labels of X and Y samples\n",
    "    Class_True = True cluster labels of samples\n",
    "    \n",
    "\"\"\"\n",
    "palette = \"Set2\"# seaborn color palette for true clusters (make sure not to use cyclic color maps)\n",
    "data_dic_orig, class_dic, num_clust, dtp = load_data_twoGRN(var_data = False, palette = palette)\n",
    "X, Y, Class_True, X_vars, Y_vars = split_data(data_dic_orig, class_dic, separation = True) # separation = False always for generate_data_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c26b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters of MIASA\n",
    "\"\"\"\n",
    "\n",
    "metric_method = (\"eCDF\", \"Granger-Cause-diff-chi2\") # (Similarity, Association) distance models\n",
    "clust_method = \"Agglomerative_ward\" # clustering aglorithm to use\n",
    "palette = \"Spectral\" # seaborn color palette\n",
    "dist_origin = (True,False) # for datasets (X, Y) decide if the distance to the origin of the axes is interpretable as the norm of the feature representations of the samples\n",
    "in_threads = False # True to avoid broken runs when using parallel jobs (relevant only for class_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be975c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If desired custom similarity feature representation (defining Euclidean similarity distance) \n",
    "and association measures then \n",
    "\n",
    "1) set \n",
    "    metric_method = \"precomputed\"\n",
    "2) give Feature_dic as parameter\n",
    "    Feature_dic[\"Feature_X\"] : array similarity features of dataset X: M samples and L features, X.shape = (M, L)\n",
    "    Feature_dic[\"Feature_Y\"] : array similarity features of dataset Y: M samples and S features, Y.shape = (N, S)\n",
    "    Feature_dic[\"Asssociation_function\"] : a function of argument tuple full datasets (X, Y) or pair of samples (X_i, Y_j) computing the pairwise association between the samples of X and Y\n",
    "    \n",
    "    Feature_dic[\"assoc_func_type\"] : type of the association function as options\n",
    "                                    option 1: str vectorized     : argument full datasets (X, Y) => return directly the Asscociation distance matrix of shape (M, N) \n",
    "                                    option 2: str not_vectorized : argument samples (X_i, Y_j)   => return a scalar = Associaiton distance between sample X_i and sample Y_j\n",
    "\n",
    "    Feature_dic[\"DMat\"]  : array distance function, then Feature_dic[\"Asssociation_function\"] Feature_dic[\"assoc_func_type\"] can be set to None \n",
    "    Feature_dic[\"dist_origin\"]: bool tuple (X?, Y?) deciding if the distance to the origin of the axes is interpretable as the norm of the feature representations of the samples\n",
    "                                Must be in aggreement with given Feature_dic[\"DMat\"]\n",
    "                                if any (X?, Y?) then shape Feature_dic[\"DMat\"].shape = (M+N+1, M+N+1) and distance to origin is placed at the M+1-th row (see function Methods.Core.CosLM.Prox_Mat)\n",
    "\n",
    "example:\n",
    "    \n",
    "from Methods.Generate_Features import eCDF, get_assoc_func\n",
    "metric_method = \"precomputed\"\n",
    "Feature_dic = {} \n",
    "Feature_dic[\"Feature_X\"], Feature_dic[\"Feature_Y\"] = eCDF(X,Y)\n",
    "Feature_dic[\"Asssociation_function\"], Feature_dic[\"assoc_func_type\"] = get_assoc_func(\"KS-stat\")\n",
    "Feature_dic[\"DMat\"] = None \n",
    "Feature_dic[\"dist_origin\"] = (True,True) ## if DMat is not None it should contain distance to origin setting \n",
    "\"\"\"\n",
    "Feature_dic = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5fe30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters of MIASA, second example and same as in the first example\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from Methods.Generate_Features import corrcoeff, Sim_GRN, Eucl, Null, eCDF, get_assoc_func\n",
    "metric_method = \"precomputed\"\n",
    "clust_method = \"Agglomerative_ward\" # clustering aglorithm to use\n",
    "Feature_dic = {}\n",
    "#from Methods.Core.Generate_Distances import Association_Distance\n",
    "#feature_X = Association_Distance((X, X), func = get_assoc_func(\"Granger-Cause-2diff-chi2\")[0], ftype = \"not vectorized\")\n",
    "#feature_Y = Association_Distance((Y, Y), func = get_assoc_func(\"Granger-Cause-2diff-chi2\")[0], ftype = \"not vectorized\")\n",
    "\n",
    "# compute moving average feature\n",
    "#feature_X = np.cumsum(X, axis = 1)/np.sum(X, axis = 1)[:, np.newaxis]\n",
    "#feature_Y = np.cumsum(Y, axis = 1)/np.sum(Y, axis = 1)[:, np.newaxis]\n",
    "#feature_X = np.zeros(X.shape)\n",
    "#ind_X = np.arange(X.shape[0]).astype(int)\n",
    "#for t in range(feature_X.shape[1]):\n",
    "#    for i in range(feature_X.shape[0]):\n",
    "#        feature_X[i, t] = np.cov(X[ind_X != i, t])\n",
    "        \n",
    "#feature_Y = np.zeros(Y.shape)\n",
    "#ind_Y = np.arange(Y.shape[0]).astype(int)\n",
    "#for t in range(feature_Y.shape[1]):\n",
    "#    for j in range(feature_Y.shape[0]):\n",
    "#        feature_X[j, t] = np.cov(Y[ind_Y != j, t])\n",
    "        \n",
    "Feature_dic[\"Feature_X\"], Feature_dic[\"Feature_Y\"] = Eucl(X, Y) \n",
    "Feature_dic[\"Asssociation_function\"], Feature_dic[\"assoc_func_type\"] = get_assoc_func(\"Granger-Cause-3diff-chi2\")\n",
    "Feature_dic[\"DMat\"] = None \n",
    "Feature_dic[\"dist_origin\"] = (True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Perform MIASA Classification of samples\n",
    "\"\"\"\n",
    "Id_Class = Miasa_Class(X, Y, num_clust, \n",
    "                       dist_origin = dist_origin, \n",
    "                       metric_method = metric_method, \n",
    "                       clust_method = clust_method, # clustering based on Euclidean embedding and point of origin is not clustered (the characterisits of the embedding makes all points too far from it and it risk to be always considered as one independen cluster)\n",
    "                       c_dic = c_dic, Feature_dic = Feature_dic,\n",
    "                       in_threads = in_threads,\n",
    "                       palette = palette)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"------------- Some checking ------------\")\n",
    "print(\"Number of True Clusters/Distributions\", len(np.unique(Class_True)), \"checked:\", num_clust == len(np.unique(Class_True)))\n",
    "num_col_true = (np.unique(pd.DataFrame(data_dic_orig[\"true_colors\"]).loc[:][:], axis = 1)).shape[1]\n",
    "print(\"Number of True colors of clusters\", num_col_true, \"checked non-cyclic:\", num_col_true == num_clust) \n",
    "\n",
    "print(\"Number of predicted clusters\", len(np.unique(Id_Class[\"Class_pred\"])), \"checked:\", num_clust == len(np.unique(Id_Class[\"Class_pred\"])))\n",
    "num_col_pred = (np.unique(Id_Class[\"color_clustered\"], axis = 0)).shape[0]\n",
    "print(\"Number of colors predicted clusters\", num_col_pred, \"checked non-cyclic:\", num_col_pred == num_clust) \n",
    "\n",
    "print(\"------------- Evaluate clustering ------------\")\n",
    "from sklearn.metrics import rand_score, adjusted_rand_score\n",
    "accuracy_2a = rand_score(Class_True, Id_Class[\"Class_pred\"])\n",
    "accuracy_3a = adjusted_rand_score(Class_True, Id_Class[\"Class_pred\"])\n",
    "\n",
    "print(\"Rand Index\", accuracy_2a, \"\\nAdjusted Rand_Index\", accuracy_3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76167c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2-Dimensional visualization of clusters (UMAP visualization) \n",
    "- all predicted classes \n",
    "- colored predicted classes or true classes \n",
    "\"\"\"\n",
    "pdf= PdfPages(\"Figures/\"+DataName+\"_UMAP.pdf\")\n",
    "fig, ax = plotClass(Id_Class, X_vars, Y_vars, pdf, dtp,\n",
    "          run_num = 1, n_neighbors = 20, min_dist = 0.99, \n",
    "          method = \"umap\", \n",
    "          scale = False, # scale = \"pca\", \"standard\", anything esle is taken as no scaling \n",
    "          cluster_colors = False, # chosed_color: if False, true_colors bellow must be given \n",
    "          true_colors = data_dic_orig[\"true_colors\"],# give a true class colors as dictionary with X_vars and Y_vars as key\n",
    "          markers = [(\" \",5),(\" \",5)], # optional markers list and their size for X and Y\n",
    "          show_labels = False, # optional show the labels of X and Y\n",
    "          show_orig = False, #optional show the the axis lines going through embedded origin \n",
    "          legend = True, # add legend only if true cluster are required\n",
    "          wrap_true = True, # wrapp the members of a true cluster , in each indentified clusters\n",
    "          group_annot_size = 15, ### size of the annotations in the center of polygonesâ€š\n",
    "          wrap_predicted = True, # full lines to wrap around the predicted cluster\n",
    "          show_pred_outliers = False, #\n",
    "          def_pred_outliers = (3, 0.75), # (a, b), greater than a*std of pairwise dist for more than b*100% of the points in the predicted class\n",
    "          oultiers_markers = (\"P\", \"^\", 5), # (true, predicted, size)\n",
    "          wrap_type = \"convexhull\", # convexhull or ellipse (ellipse does not look accurate)\n",
    "          dataname = \"GRN\") # true cluster markers for this simulation\n",
    "import matplotlib.pyplot as plt\n",
    "pdf.savefig(fig, bbox_inches = \"tight\")\n",
    "plt.legend(loc = (0,1), fontsize = 9, ncol = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2-Dimensional visualization visualization of clusters (UMAP visualization) \n",
    "- separated predicted classes \n",
    "- colored true classes\n",
    "\"\"\"\n",
    "fig, ax = plotClass_separated(Id_Class, X_vars, Y_vars, pdf, dtp,\n",
    "          run_num = 1, n_neighbors = 50, min_dist = 0.99, \n",
    "          method = \"umap\",\n",
    "          scale = False, # scale = \"pca\", \"standard\", anything esle is taken as no scaling \n",
    "          cluster_colors = False, # if False, true_colors bellow must be given \n",
    "          true_colors = data_dic_orig[\"true_colors\"], # give a true class colors as dictionary with X_vars and Y_vars as key\n",
    "          markers = [(\"o\",100),(\"^\",100)], # optional markers list and their size for X and Y\n",
    "          sub_fig_size = 10, # optional sub figure size (as a square)\n",
    "          show_labels = True, # optional show the labels of X and Y\n",
    "          show_orig = False, # optional show the the axis lines going through origin \n",
    "          show_separation = True, # optional separate all subfigs\n",
    "          num_row_col = (2, 2),  # number of subfigs in row and col\n",
    "          dataname = \"GRN\") # true cluster markers for this simulation\n",
    "pdf.close() # save everything \n",
    "plt.show()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999be037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2-Dimensional visualization visualization of clusters (UMAP visualization) \n",
    "- separated predicted classes \n",
    "- colored true classes\n",
    "\"\"\"\n",
    "pdf = PdfPages(\"Figures/\"+DataName+\"_UMAP.pdf\")\n",
    "fig, ax = plotClass_separated(Id_Class, X_vars, Y_vars, pdf, dtp,\n",
    "          run_num = 1, n_neighbors = 50, min_dist = 0.99, \n",
    "          method = \"umap\",\n",
    "          scale = False, # scale = \"pca\", \"standard\", anything esle is taken as no scaling \n",
    "          cluster_colors = False, # if False, true_colors bellow must be given \n",
    "          true_colors = data_dic_orig[\"true_colors\"], # give a true class colors as dictionary with X_vars and Y_vars as key\n",
    "          markers = [(\"o\",100),(\"^\",100)], # optional markers list and their size for X and Y\n",
    "          sub_fig_size = 10, # optional sub figure size (as a square)\n",
    "          show_labels = False, # optional show the labels of X and Y\n",
    "          show_orig = False, # optional show the the axis lines going through origin \n",
    "          show_separation = True, # optional separate all subfigs\n",
    "          num_row_col = (2, 2) )  # number of subfigs in row and col\n",
    "\n",
    "pdf.savefig(fig, bbox_inches = \"tight\")\n",
    "pdf.close() # save everything \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef13723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2-Dimensional visualization of clusters (TSNE visualization) \n",
    "- all predicted classes \n",
    "- colored predicted classes \n",
    "\"\"\"\n",
    "pdf= PdfPages(\"Figures/\"+DataName+\"_TSNE.pdf\")\n",
    "fig, ax = plotClass(Id_Class, X_vars, Y_vars, pdf, dtp,\n",
    "          run_num = 1, n_neighbors = 30, \n",
    "          method = \"t-SNE\", scale = False, # scale = \"pca\", \"standard\", anything esle is taken as no scaling \n",
    "          cluster_colors =  True, # chosed_color: if False, true_colors bellow must be given \n",
    "          true_colors =  False,# give a true class colors as dictionary with X_vars and Y_vars as key\n",
    "          markers = [(\"o\",20),(\"s\",20)], # optional markers list and their size for X and Y\n",
    "          show_labels = False, # optional show the labels of X and Y\n",
    "          show_orig = True)# optional show the the axis lines going through origin \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2-Dimensional visualization of MIASA clusters (TSNE visualization) \n",
    "- separated predicted classes \n",
    "- colored true classes\n",
    "\"\"\"\n",
    "fig, ax = plotClass_separated(Id_Class, X_vars, Y_vars, pdf, dtp,\n",
    "          run_num = 1, n_neighbors = 30, \n",
    "          method = \"t-SNE\", \n",
    "          scale = False, # scale = \"pca\", \"standard\", anything esle is taken as no scaling \n",
    "          cluster_colors = False, # chosed_color: if False, true_colors bellow must be given \n",
    "          true_colors = data_dic_orig[\"true_colors\"], # give a true class colors as dictionary with X_vars and Y_vars as key\n",
    "          markers = [(\"o\",100),(\"s\",100)], # optional markers list and their size for X and Y\n",
    "          sub_fig_size = 10, # optional sub figure size (as a square)\n",
    "          show_labels = False, # optional show the labels of X and Y\n",
    "          show_orig = False, # optional show the the axis lines going through origin\n",
    "          show_separation = True, # optional separate all subfigs\n",
    "          num_row_col = (20, 2) )  # number of subfigs in row and col\n",
    "pdf.close() # save everything                \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform analogue non-metric: extract the classification, \n",
    "MDS methods based on the distance matrix can be used for visualization (e.g. sklearn.manifold.MDS or t-SNE). \n",
    "\"\"\"\n",
    "from Methods.NonMD_class import NonMetric_Class\n",
    "Id_Class_2 = NonMetric_Class(X, Y, num_clust, \n",
    "                       dist_origin = dist_origin,  # including distance to origin is the same as clustering over true feature norm\n",
    "                       metric_method = metric_method, \n",
    "                       clust_method = \"Kmedoids\", # clustering is based on distance matrix including info in dist_origin (it is not Euclidean thus we can't use Kmeans)\n",
    "                        Feature_dic = Feature_dic,\n",
    "                       in_threads = in_threads,\n",
    "                       palette = palette)\n",
    "\n",
    "print(\"------------- Evaluate clustering ------------\")\n",
    "accuracy_2b = rand_score(Class_True, Id_Class_2[\"Class_pred\"])\n",
    "accuracy_3b = adjusted_rand_score(Class_True, Id_Class_2[\"Class_pred\"])\n",
    "print(\"Rand Index\", accuracy_2b, \"\\nAdjusted Rand_Idex\", accuracy_3b)\n",
    "\n",
    "\"\"\"\n",
    "2-Dimensional visualization of clusters (MDS visualization) \n",
    "- all predicted classes \n",
    "- colored predicted classes \n",
    "\"\"\"\n",
    "nonMD_low = \"t-SNE\"\n",
    "pdf = PdfPages(\"Figures/\"+DataName+\"_NonMD_%s.pdf\"%nonMD_low)\n",
    "fig, ax = plotClass(Id_Class, X_vars, Y_vars, pdf, dtp,\n",
    "          run_num = 1, n_neighbors = 30,\n",
    "          method = nonMD_low, \n",
    "          scale = False, # scale = \"pca\", \"standard\", anything esle is taken as no scaling \n",
    "          cluster_colors = True, # chosed_color: if False, true_colors bellow must be given \n",
    "          true_colors = False,# give a true class colors as dictionary with X_vars and Y_vars as key\n",
    "          markers = [(\"o\",20),(\"s\",20)], # optional markers list and their size for X and Y\n",
    "          show_labels = False, # optional show the labels of X and Y\n",
    "          show_orig = True, # optional show the the axis lines going through origin  \n",
    "          metric = \"precomputed\") # Necessary option for non-Metric, use Distance matrix\n",
    "pdf.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clust_method_2 = \"Simple_Min_Dist\"\n",
    "from Methods.NonMD_class import NonMetric_Class\n",
    "from sklearn.metrics import accuracy_score\n",
    "#clust_method_2 = MLPClassifier, (len(X), len(Y), Class_True, 75) # NN Classifier learns from distance matrix, requires true classes and we use x%(e.g:0.75) of original data as training set\n",
    "\"\"\"\n",
    "clust_method_2b = \"Kmedoids\"\n",
    "Id_Class_2b = NonMetric_Class(X, Y, num_clust, \n",
    "                       dist_origin = dist_origin, \n",
    "                       metric_method = metric_method, \n",
    "                       clust_method = clust_method_2b, \n",
    "                        Feature_dic = Feature_dic,\n",
    "                       in_threads = in_threads,\n",
    "                       palette = palette)\n",
    "\n",
    "accuracy_1b = accuracy_score(Class_True, Id_Class_2b[\"Class_pred\"])\n",
    "accuracy_2b = rand_score(Class_True, Id_Class_2b[\"Class_pred\"])\n",
    "accuracy_3b = adjusted_rand_score(Class_True, Id_Class_2b[\"Class_pred\"])\n",
    "\n",
    "print(\"NN Classifier Accuracy:\", accuracy_1b, \"\\nRand Index\", accuracy_2b, \"\\nAdjusted Rand_Index\", accuracy_3b)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebe7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.educative.io/answers/implement-neural-network-for-classification-using-scikit-learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "# https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a32e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Evaluating the results of the model\n",
    "accuracy = accuracy_score(y_test,y_pred)*100\n",
    "confusion_mat = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Printing the Results\n",
    "print(\"Accuracy for Neural Network is:\",accuracy)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macPy3",
   "language": "python",
   "name": "macpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
